{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statsmodels.tools.eval_measures import bic\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import dblquad\n",
    "import itertools\n",
    "from scipy.special import logsumexp, rel_entr\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import scipy.optimize as optimization\n",
    "\n",
    "# for debugging\n",
    "from collections import Counter\n",
    "\n",
    "# for data generation\n",
    "from sklearn.datasets import make_classification\n",
    "from math import ceil, log\n",
    "\n",
    "# suppress warning: warnings are only thrown by ICA if ICA cannot converge. In this case,\n",
    "# the data is already Gaussian (i.e., that's a positive outcome!)\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Kernel Density Estimation (from Imitate paper)'''\n",
    "class DE_kde():\n",
    "    def __init__(self, num_bins):\n",
    "        self.num_bins = num_bins\n",
    "        \n",
    "    def estimate(self, data, d_min, d_max, weights):\n",
    "        d_range = (d_min - 0.5*(d_max-d_min), d_max + 0.5*(d_max-d_min))\n",
    "        gridsize = (d_range[1] - d_range[0]) / self.num_bins\n",
    "\n",
    "        self.grid = [(d_range[0] + i*gridsize) for i in range(self.num_bins+1)]\n",
    "        self.mids = self.grid[:-1] + np.diff(self.grid)/2\n",
    "        \n",
    "        kde = KDEUnivariate(data)\n",
    "        try:\n",
    "            kde.fit(bw='silverman', kernel='gau', fft=False, weights=weights)\n",
    "        except:\n",
    "            kde.fit(bw=0.01, kernel='gau', fft=False, weights=weights)\n",
    "        \n",
    "        self.values = [kde.evaluate(i)[0] if kde.evaluate(i) > 0 else 0 for i in self.mids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Scaled normal distribution (from Imitate paper)'''\n",
    "class scaled_norm():\n",
    "    def __init__(self, truncate_std=3, ends_zero=True, print_loss=False):\n",
    "        self.ends_zero = ends_zero\n",
    "        self.print_loss = print_loss\n",
    "        self.truncate_std = truncate_std\n",
    "        \n",
    "    def func_trunc(x, scale, mu, sigma, trunc):\n",
    "        res = scaled_norm.func(x, scale, mu, sigma)\n",
    "        res[abs(x - mu) > trunc*sigma] = 0\n",
    "        return res\n",
    "        \n",
    "    def func(x, scale, mu, sigma):\n",
    "        return scale * norm(mu, sigma).pdf(x)\n",
    "    \n",
    "    def weighted_dist(weights, points_x, points_y, params):\n",
    "        return (((scaled_norm.func(points_x, *params) - points_y)**2) * weights).sum()\n",
    "    \n",
    "    def constraint(points_x, points_y, params):\n",
    "        return 2*points_y.sum() - scaled_norm.func(points_x, *params).sum()\n",
    "\n",
    "    def fit(self, points_x, points_y, data, returnParams=False):\n",
    "        d_mean = points_x[np.argmax(points_y)]  # highest bin\n",
    "        d_std = max(0.0001, np.sqrt( np.sum((np.array(data) - d_mean)**2) / (len(data) - 1) ))\n",
    "        d_scale = max(points_y) / max(scaled_norm.func(points_x, 1, d_mean, d_std))\n",
    "        p0 = np.array([d_scale, d_mean, d_std])  # initial parameters\n",
    "        weights = np.array(points_y) ** 2\n",
    "        weights = [max(weights[i], 0.01*max(points_y)) for i in range(len(weights))]\n",
    "        optimize_me = lambda p: scaled_norm.weighted_dist(weights, points_x, points_y, p)\n",
    "        if self.ends_zero:\n",
    "            weights[0] = weights[-1] = max(points_y)\n",
    "        try:\n",
    "            bounds = [[0.01, 2*d_scale], [points_x[0], points_x[-1]], [0.0001,(points_x[-1]-points_x[0])/2]]\n",
    "            constr = lambda p: scaled_norm.constraint(np.array(points_x), np.array(points_y), p)\n",
    "            res = minimize(optimize_me, p0, method='SLSQP', bounds=bounds, constraints={'type':'ineq', 'fun': constr})\n",
    "            if constr(res.x) < 0:\n",
    "                if returnParams:  return np.array(points_y), p0\n",
    "                else: return np.array(points_y)\n",
    "            if self.print_loss:\n",
    "                print(\"final weighted loss:\", optimize_me(res.x))\n",
    "            if returnParams: return scaled_norm.func_trunc(points_x, *res.x, self.truncate_std), res.x\n",
    "            else: return scaled_norm.func_trunc(points_x, *res.x, self.truncate_std)\n",
    "        except:\n",
    "            #print(\"pdf fitting with sigma was not successful\")\n",
    "            if returnParams:  return np.array(points_y), p0\n",
    "            else: return np.array(points_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_lof(data, k=10):\n",
    "    k = min((len(data), k))\n",
    "    lof = LocalOutlierFactor(n_neighbors=k)\n",
    "    stays = lof.fit_predict(data)\n",
    "    return np.array(data)[stays == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClusters(data, labels, title=\"\"):\n",
    "    inds = labels.argsort()\n",
    "    data_sorted = data[inds]\n",
    "\n",
    "    plt.scatter(data_sorted[:,0], data_sorted[:,1], c=labels[inds], alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBias(data, labels, deleted, title=\"\"):\n",
    "    plt.scatter(data[:,0], data[:,1], c=labels, alpha=0.5)\n",
    "    plt.scatter(data[deleted,0], data[deleted,1], facecolors='None', edgecolors='r', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the number of bins that best describe the data (AICc)\n",
    "def getBestNumBins(bins, d):\n",
    "    min_aicc = np.Inf\n",
    "    best_bins = 0\n",
    "    \n",
    "    for num_bins in bins:\n",
    "        # get histogram\n",
    "        d_range = (min(d) - 0.5*(max(d)-min(d)), max(d) + 0.5*(max(d)-min(d)))\n",
    "        values, grid = np.histogram(d, bins=num_bins, density=True, range=d_range)\n",
    "        mids = grid[:-1] + np.diff(grid)/2\n",
    "\n",
    "        # calc MLE: ln(P[data | model])\n",
    "        bin_per_p = np.digitize(d, grid)\n",
    "        ln_L = sum(np.log( values[bin_per_p - 1] ))\n",
    "\n",
    "        # calc aicc\n",
    "        aicc = 2*num_bins*len(d) / (len(d)-num_bins-1) - 2*ln_L\n",
    "        #bic =  log(len(d))*num_bins - 2*ln_L\n",
    "        #bic =  2*num_bins - 2*ln_L # aic\n",
    "        \n",
    "        if aicc < min_aicc:\n",
    "            min_aicc = aicc\n",
    "            best_bins = num_bins\n",
    "\n",
    "    return best_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findK(data):\n",
    "    sample, _, _, _ = train_test_split(data, [0]*len(data), train_size=min(500, int(0.7*len(data))))\n",
    "    max_num_test = int(len(data) / 100)\n",
    "    scores = np.array([-1.0]*(max_num_test+1))\n",
    "    for n in range(2, max_num_test+1):\n",
    "        scores[n] = silhouette_score(sample, KMeans(n_clusters=n, random_state=0).fit_predict(sample))\n",
    "    return int(max(5, np.argmax(scores)*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_with_kmeans(data_full, data_clean, k=100, plots=False):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10).fit(np.array(data_clean))\n",
    "    lab = kmeans.predict(data_full)\n",
    "    if plots:\n",
    "        plotClusters(data_full, lab)\n",
    "        \n",
    "    for cl in np.unique(lab):\n",
    "        idcs = np.where(lab == cl)[0]\n",
    "        if len(idcs) < 10: continue\n",
    "        if split_test(data_full[idcs]):\n",
    "            km = KMeans(n_clusters=2, n_init=10).fit(data_full[idcs])\n",
    "            l = km.predict(data_full[idcs])\n",
    "            lab[idcs[l==1]] = max(lab)+1\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test(cluster):\n",
    "    if np.sum(np.std(cluster, axis=0) == 0) > 0: \n",
    "        cluster = cluster[:,np.std(cluster, axis=0)>0]\n",
    "    ica = FastICA(n_components=len(cluster[0]), max_iter=500, tol=0.01).fit(cluster)\n",
    "    data_trf = ica.transform(cluster)\n",
    "    for d in range(len(cluster[0])):\n",
    "        DE = DE_kde(30)    # define Density Estimator\n",
    "        DE.estimate(data_trf[:,d], min(data_trf[:,d]), max(data_trf[:,d]), weights=np.ones(len(data_trf[:,d])))\n",
    "        hh = DE.values\n",
    "        for i in range(1, len(hh)-1):\n",
    "            if max(hh[0:i]) > 0.05*max(hh)+hh[i] and max(hh[i+1:]) > 0.05*max(hh)+hh[i]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitate Fitting (no ICA, no generation)\n",
    "This is our re-implementation, not the original Imitate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Imitate(data, plots=False, print_loss=False, num_bins=0):\n",
    "    grids = []\n",
    "    fitted = []\n",
    "    fill_up = []\n",
    "    num_fill_up = []\n",
    "    params = []\n",
    "\n",
    "    # consider every dimension\n",
    "    for line in range(len(data[0])):\n",
    "        \n",
    "        d = data[:,line]   # project onto line\n",
    "        if num_bins==0: num_bins = getBestNumBins(range(min(int(len(d)/2),20), min(60, len(d)-2)), d)\n",
    "        \n",
    "        d_range = (min(d) - 0.5*(max(d)-min(d)), max(d) + 0.5*(max(d)-min(d)))\n",
    "        grid = [(d_range[0] + i*((d_range[1] - d_range[0]) / num_bins)) for i in range(num_bins+1)]\n",
    "        mids = grid[:-1] + np.diff(grid)/2\n",
    "        kde = KDEUnivariate(d)\n",
    "        try:\n",
    "            kde.fit(bw='silverman', kernel='gau', fft=False)      \n",
    "        except:\n",
    "            kde.fit(bw=0.01, kernel='gau', fft=False)  \n",
    "        values = np.array([kde.evaluate(i)[0] if kde.evaluate(i) > 0 else 0 for i in mids])\n",
    "        values_scaled = (len(d) / sum(values)) * values    # scale to absolute values\n",
    "        grids.append(grid)\n",
    "\n",
    "        SN = scaled_norm()\n",
    "        fitted_, p = SN.fit(mids, values_scaled, d, returnParams=True) # fit Gaussian   \n",
    "        params.append(p)  \n",
    "        fitted.append(fitted_)\n",
    "\n",
    "        diff = fitted_ - values_scaled    # decide where to fill up\n",
    "        diff[diff < 1] = 0 # don't fill if we are not sure that we need the point\n",
    "        fill_up.append(np.floor(diff).astype(int))\n",
    "\n",
    "        num_fill_up.append(sum(np.floor(diff)))    # count how much needs to be filled\n",
    "        \n",
    "        if plots:\n",
    "            plt.bar(mids, values_scaled, label='true data', width=(mids[1]-mids[0]))\n",
    "            plt.bar(mids, diff, bottom=values_scaled, label='fill up', width=(mids[1]-mids[0]))\n",
    "            plt.plot(mids, fitted_, label='fitted', c='red')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    mean = np.array(params)[:,1]\n",
    "    cov = np.zeros((len(data[0]), len(data[0])))\n",
    "    np.fill_diagonal(cov, np.array(params)[:,2]**2)\n",
    "\n",
    "    return grids, fitted, fill_up, num_fill_up, mean, cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mimic(full_data, init_labels=None, k_init=100, cluster_plots=True, full_plots=False):\n",
    "    \n",
    "    # remove outliers\n",
    "    lof = LocalOutlierFactor(n_neighbors=min((len(full_data), 10)))\n",
    "    valid = lof.fit_predict(full_data)\n",
    "    lof_nf = lof.negative_outlier_factor_[valid==1]\n",
    "    \n",
    "    # initialize using KMeans with a very large number of clusters\n",
    "    if init_labels is None:\n",
    "        init_labels = init_with_kmeans(full_data, full_data[valid==1], k_init, cluster_plots)\n",
    "    \n",
    "    data = full_data[valid==1] # clean data\n",
    "    labels = init_labels[valid==1]\n",
    "    \n",
    "    lof_nf_per_cluster = np.array([np.average(lof_nf[labels==c]) for c in np.unique(labels)])\n",
    "    lof_nf_min = np.mean(lof_nf_per_cluster) - 3*np.std(lof_nf_per_cluster)\n",
    "    \n",
    "    clusters_checked = [] # indicate which cluster have already been processed\n",
    "    labels_largest_clust = np.copy(labels) # used for the largest cluster count\n",
    "    cluster_assignments = np.empty((len(full_data),0)) # store the output probabilities per cluster\n",
    "    \n",
    "    parameters = [] #store parameters and ICAs\n",
    "    \n",
    "    while True:\n",
    "        # \"reset\" labels\n",
    "        labels_tmp = np.copy(labels)\n",
    "        \n",
    "        # get largest cluster\n",
    "        cluster_counts = Counter(labels_largest_clust)\n",
    "        #cluster_counts = Counter(labels_tmp[valid==1])\n",
    "        for key in clusters_checked:\n",
    "            if key in cluster_counts:\n",
    "                _ = cluster_counts.pop(key) \n",
    "        if -1 in cluster_counts: #-1 is reserved for outliers!\n",
    "            _ = cluster_counts.pop(-1)\n",
    "        \n",
    "        # if no cluster left: stop right away\n",
    "        if len(cluster_counts) == 0:\n",
    "            break\n",
    "        largest_cluster = max(cluster_counts, key=cluster_counts.get)\n",
    "        \n",
    "        # if small: stop right away\n",
    "        if cluster_counts[largest_cluster] < 10:\n",
    "            clusters_checked = np.append(clusters_checked, [largest_cluster])\n",
    "            break\n",
    "            \n",
    "        # if assembly of \"left-overs\": skip\n",
    "        #print(\"Avg LOF score in cluster:\", np.average(lof_nf[labels_largest_clust == largest_cluster]),\n",
    "        #     \"min acceptable: \", lof_nf_min)\n",
    "        if np.average(lof_nf[labels_largest_clust == largest_cluster]) < lof_nf_min:\n",
    "            clusters_checked = np.append(clusters_checked, [largest_cluster])\n",
    "            continue\n",
    "        \n",
    "        # run iteration, receive labels_tmp and probs_clust for this cluster\n",
    "        labels_tmp, probs_clust, orig_mean, orig_cov = grow_cluster(data, full_data, labels_tmp, largest_cluster, full_plots)\n",
    "        \n",
    "        # overwrite labels with the newly formed cluster where applicable\n",
    "        labels_largest_clust[labels_tmp == largest_cluster] = largest_cluster\n",
    "        clusters_checked = np.append(clusters_checked, [largest_cluster])\n",
    "        \n",
    "        # append estimated probs per point for the constructed cluster to cluster_assignments\n",
    "        cluster_assignments = np.column_stack((cluster_assignments, np.array(probs_clust)))\n",
    "        parameters.append([orig_mean, orig_cov])\n",
    "        \n",
    "        if cluster_plots:\n",
    "            print(\"Plotting cluster \", largest_cluster, \"here:\")\n",
    "            plotClusters(data, labels_tmp==largest_cluster)\n",
    "    \n",
    "    return cluster_assignments, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mimic: Grow cluster: $P[\\text{model}|\\text{data}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_model_given_data(data_trf, cl_idcs, add_to_cluster_idcs, grids, fitted, p_data_incr=0):\n",
    "    idcs = np.append(cl_idcs, add_to_cluster_idcs).astype(int)\n",
    "    \n",
    "    p_data_given_model = P_data_given_model(data_trf[idcs], grids, fitted)\n",
    "    if p_data_incr > 0:\n",
    "        p_data = p_data_incr + P_data(data_trf[add_to_cluster_idcs], grids, fitted, s=3)\n",
    "    else: \n",
    "        p_data = P_data(data_trf[idcs], grids, fitted, s=3)\n",
    "    p_model = 1 #same model for all datasets, so no need to calculate that!\n",
    "    p_model_given_data = p_data_given_model - p_data\n",
    "    #print(\"p_data_given_model - p_data = %.2f - %.2f = %.2f\" % (p_data_given_model, p_data, p_model_given_data))\n",
    "    return p_model_given_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_data_given_model(data, grids, fitted):\n",
    "    res = 0\n",
    "    for d in range(len(data[0])):\n",
    "        fitted[d][fitted[d]==0] = 0.00001  #use non-truncated results later!!\n",
    "        hh = np.histogram(data[:,d], bins=grids[d])[0] #histogram heights\n",
    "        f = fitted[d] / np.sum(fitted[d])\n",
    "        tmp = hh * np.log(f)\n",
    "        tmp[hh==0] = 0 # 0 times whatever should be 0\n",
    "        res += np.sum(tmp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_data(data, grids, fitted, s=3):\n",
    "    mean = np.array([(g[0] + g[-1])/2.0 for g in grids])\n",
    "    std = (mean - np.array([g[0] for g in grids])) / 3.0\n",
    "    cov = np.zeros((len(grids), len(grids)))\n",
    "    np.fill_diagonal(cov, std**2)\n",
    "    \n",
    "    mn = multivariate_normal(mean, cov)#.pdf(data)\n",
    "    mids = [(np.array(grids[d][0:-1]) + np.array(grids[d][1:])) / 2 for d in range(len(grids))]\n",
    "    cubesize = np.prod(np.array([g[1] - g[0] for g in grids]))\n",
    "    \n",
    "    data_in_bins = np.column_stack([np.digitize(data[:,i], grids[i])-1 for i in range(len(grids))])\n",
    "    corresp_mids = np.column_stack([mids[i][data_in_bins[:,i]] for i in range(len(grids))])\n",
    "    probs = mn.pdf(corresp_mids) * cubesize\n",
    "    \n",
    "    return np.sum(np.log(probs))\n",
    "    #return np.sum(np.log(mn))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(data[0])>5: return -len(data)\n",
    "    # (1 / #gridcells) ^ #datapoints\n",
    "    # log  ->  #data * (- log(#cells)) = - #data * log(#cells) -> - #data because #cells is constant here\n",
    "    #log_cells = sum(np.log([len(grids[i]) for i in range(len(grids))]))\n",
    "    #return - len(data) * log_cells\n",
    "    mids = [(np.array(grids[d][0:-1]) + np.array(grids[d][1:])) / 2 for d in range(len(grids))]\n",
    "\n",
    "    mu0 = np.array([g[0] for g in grids]) # lower mu boundaries for all dimensions\n",
    "    mu1 = np.array([g[-1] for g in grids]) # upper mu boundaries for all dimensions\n",
    "    sig0 = np.array([0.01] * len(mu0)) # lower sig boundaries for all dimensions\n",
    "    sig1 = (mu1 - mu0)*0.5 # upper sig boundaries for all dimensions\n",
    "    mu_step = (mu1 - mu0) / s # mu step size per dimension\n",
    "    sig_step = (sig1 - sig0) / s # sig step size per dimension\n",
    "    \n",
    "    # split into little boxes: dim x boxes\n",
    "    mu = [np.linspace(mu0[d] + 0.5*mu_step[d], mu1[d] - 0.5*mu_step[d], s) for d in range(len(mu0))]\n",
    "    sig = [np.linspace(sig0[d] + 0.5*sig_step[d], sig1[d] - 0.5*sig_step[d], s) for d in range(len(sig0))]\n",
    "    #ln_hypercube_size = np.sum(np.log(mu_step + 1)) + np.sum(np.log(sig_step + 1))\n",
    "    ln_hypercube_size = np.sum(np.log(mu_step)) + np.sum(np.log(sig_step))\n",
    "    # param list: mu for dim0, sig for dim 0, mu for dim 1, sig for dim 1, ...\n",
    "    params_per_d = list(itertools.chain.from_iterable(zip(mu, sig)))\n",
    "\n",
    "    coords = np.meshgrid(*params_per_d)\n",
    "    param_tuples = np.column_stack([coords[i].flatten() for i in range(len(coords))])\n",
    "    \n",
    "    # param combos for all dimensions (it's a 2d-tuple with #dims=:d)\n",
    "    vals = []\n",
    "    for p_2dtuple in param_tuples: #(mu_dim0, sig_dim0, mu_dim1, sig_dim1, ...)\n",
    "        val = ln_hypercube_size\n",
    "\n",
    "        mus = [p_2dtuple[2*i] for i in range(int(len(p_2dtuple)/2))] #(mu_dim0, mu_dim1, ...)\n",
    "        sigs = [p_2dtuple[2*i+1] for i in range(int(len(p_2dtuple)/2))] #(sig_dim0, sig_dim1, ...)\n",
    "        # build up \"fitted\" here and then send together with grids! \n",
    "        fitted_p = [norm(mus[d],sigs[d]).pdf(mids[d]) for d in range(len(mids))]\n",
    "        val += P_data_given_model(data, grids, fitted_p)\n",
    "\n",
    "        # split into 2-tuples for each dimension\n",
    "        p_2tuples = p_2dtuple.reshape(int(len(p_2dtuple)/2), 2)\n",
    "        #iterate over dimensions\n",
    "        for d in range(len(p_2tuples)):\n",
    "            p = p_2tuples[d]\n",
    "            val += log(P_model(p[0], mu0[d], mu1[d])) + log(P_model(p[1], sig0[d], sig1[d]))\n",
    "        vals.append(val)\n",
    "        \n",
    "    return logsumexp(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_model(x, left, right):\n",
    "    mu = (left + right) / 2\n",
    "    sigma = (mu - left) / 3\n",
    "    res = norm(mu, sigma).pdf(x)\n",
    "    #res[abs(x - mu) > 3*sigma] = 0\n",
    "    return np.prod(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mimic: Grow cluster (eat points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_cluster(data, full_data, labels_tmp, grow_this, plots=False):\n",
    "    \n",
    "    if plots:\n",
    "        plotClusters(data, labels_tmp==grow_this)\n",
    "\n",
    "    cont_loop = True\n",
    "    #mle = -np.Inf\n",
    "    while cont_loop:\n",
    "        # get the points with this label\n",
    "        cl_idcs = np.where(labels_tmp==grow_this)[0]\n",
    "        if len(data) == len(cl_idcs): # break if we already sucked in all data\n",
    "            break\n",
    "    \n",
    "        add_to_cluster_idcs, grids, fitted, ica, mean, cov = eat_points(data, cl_idcs, plots)\n",
    "        # update labels\n",
    "        labels_tmp[add_to_cluster_idcs] = grow_this\n",
    "        cont_loop = len(add_to_cluster_idcs) > 0 # break if we don't add any points\n",
    "    \n",
    "    # use the final fit to assign probabilities to all data points\n",
    "    probs_clust = getProbs(ica.transform(full_data), grids, fitted)\n",
    "    \n",
    "    # transform parameters back to original space\n",
    "    orig_cov = ica.mixing_.dot(cov).dot(ica.mixing_.transpose())\n",
    "    orig_mean = ica.inverse_transform([mean])[0]\n",
    "    \n",
    "    return labels_tmp, probs_clust, orig_mean, orig_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy_ICA():\n",
    "    def __init__(self, num_dims):\n",
    "        self.mixing_ = np.identity(num_dims)\n",
    "    def transform(data):\n",
    "        return data\n",
    "    def inverse_transform(data):\n",
    "        return [data, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eat_points(data, cl_idcs, plots=False, batchsize=10):\n",
    "    \n",
    "    # train ICA on cluster, transform everything\n",
    "    cluster_clean = remove_outliers_lof(data[cl_idcs].astype(float))\n",
    "    try:\n",
    "        dev = np.std(cluster_clean, axis=0)\n",
    "        if np.sum(dev == 0) > 0:\n",
    "            for i in np.where(dev==0)[0]:\n",
    "                cluster_clean[:,i] += np.random.normal(0,1,len(cluster_clean))\n",
    "        ica = FastICA(n_components=len(data[0]), max_iter=500, tol=0.01).fit(cluster_clean.astype(float))\n",
    "    except:\n",
    "        ica = Dummy_ICA(len(data[0]))\n",
    "    data_trf = ica.transform(data)\n",
    "    \n",
    "    # use Imitate\n",
    "    grids, fitted, fill_up, num_fill_up, mean, cov = Imitate(data_trf[cl_idcs], plots=plots)\n",
    "    \n",
    "    # score all points\n",
    "    score = score1(data_trf, grids, fitted, fill_up)\n",
    "    score[cl_idcs] = 0 #don't add the points already in the cluster\n",
    "    if np.sum(score) == 0: return [], grids, fitted, ica, mean, cov\n",
    "    score = score / np.sum(score)  #convert to probability distribution\n",
    "    \n",
    "    # greedily add points and see if that improves likelihood\n",
    "    add_to_cluster_idcs = np.array([], dtype=np.int32)\n",
    "    p_model_given_data = P_model_given_data(data_trf, cl_idcs, add_to_cluster_idcs, grids, fitted)\n",
    "    num_fill = int(min(max(num_fill_up), len(score>0)))\n",
    "    batches = np.append([batchsize] * (num_fill // batchsize), [num_fill % batchsize])\n",
    "    tries = 0\n",
    "    for i in range(len(batches)):\n",
    "        candidates = np.random.choice(range(len(score)), int(batches[i]), p=score).astype(int)\n",
    "        P_new = P_model_given_data(data_trf, cl_idcs, np.append(add_to_cluster_idcs, candidates), grids, \n",
    "                                   fitted, p_data_incr=p_model_given_data)\n",
    "        if P_new <= p_model_given_data: # stopping if likelihood gets worse\n",
    "            if tries < 3:\n",
    "                i += -1 # try again!\n",
    "                tries += 1\n",
    "            else:\n",
    "                tries = 0\n",
    "            continue\n",
    "        p_model_given_data = P_new\n",
    "        add_to_cluster_idcs = np.append(add_to_cluster_idcs, candidates)\n",
    "        score[add_to_cluster_idcs] = 0\n",
    "        if np.sum(score) == 0: break\n",
    "        score = score / np.sum(score)  #convert to probability distribution\n",
    "        \n",
    "    if plots:\n",
    "        plot_labels = np.zeros(len(data))\n",
    "        plot_labels[cl_idcs] = 1\n",
    "        plot_labels[add_to_cluster_idcs] = 2\n",
    "        plotClusters(data, plot_labels)\n",
    "\n",
    "    return add_to_cluster_idcs, grids, fitted, ica, mean, cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mimic: Grow cluster: Score candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score1(data_trf, grids, fitted, fill_up):  \n",
    "    #  assign grid cell to data point and get corresponding entry in \"fitted\" or 0\n",
    "    fitted_grid = np.zeros((len(data_trf), len(data_trf[0]))) # points x dims\n",
    "    fill_grid = np.zeros((len(data_trf), len(data_trf[0]))) # points x dims\n",
    "    for d in range(len(data_trf[0])):\n",
    "        # organize in grid cells: 0 = smaller; len(grids[0]) = larger\n",
    "        grid_dim = np.digitize(data_trf[:,d], grids[d]) # points x dims\n",
    "        map_to_fitted = np.vectorize(lambda idx: 0 if idx<=0 or idx>=len(grids[d]) else fitted[d][idx-1])\n",
    "        map_to_fill = np.vectorize(lambda idx: 0 if idx<=0 or idx>=len(grids[d]) else fill_up[d][idx-1])\n",
    "        fitted_grid[:, d] = map_to_fitted(grid_dim)\n",
    "        fill_grid[:, d] = map_to_fill(grid_dim)\n",
    "        \n",
    "    s1 = np.sum(np.log(fitted_grid + 1), axis=1)  # fitted distribution\n",
    "    s2 = np.sum(np.log(fill_grid + 1), axis=1)   # fill_up\n",
    "    s = s1 + len(data_trf[0])*s2   # score as the sum of both (weighted?)\n",
    "    s[np.sum(fill_grid, axis=1) == 0] = 0   # 0 score where we don't fill anything up\n",
    "    s[np.prod(fitted_grid, axis=1) == 0] = 0   # 0 score for unprobable entries\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbs(data_tf, grids, fitted):\n",
    "\n",
    "    #  assign grid cell to data point and get corresponding entry in \"fitted\" or 0\n",
    "    fitted_grid = np.zeros((len(data_tf), len(data_tf[0]))) # points x dims\n",
    "    for d in range(len(data_tf[0])):\n",
    "        # organize in grid cells: 0 = smaller; len(grids[0]) = larger\n",
    "        grid_dim = np.digitize(data_tf[:,d], grids[d]) # points x dims\n",
    "        map_to_fitted = np.vectorize(lambda idx: 0 if idx<=0 or idx>=len(grids[d]) else fitted[d][idx-1])\n",
    "        fitted_grid[:, d] = map_to_fitted(grid_dim)\n",
    "    #  normalize (divide by sum(fitted) per dimension), average over dimensions\n",
    "    sum_fitted_per_dim = [sum(fitted[i]) for i in range(len(fitted))]\n",
    "    fitted_grid = fitted_grid / sum_fitted_per_dim\n",
    "    #probs_clust = np.sum(np.log(fitted_grid), axis=1) \n",
    "    probs_clust = fitted_grid.prod(1)\n",
    "    \n",
    "    return probs_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated probabilities using mahalanobis distance\n",
    "def getProbs_param(data, params, truncate=0):\n",
    "    mu, Cov = params\n",
    "    x = multivariate_normal(mu, Cov).pdf(data)\n",
    "    if truncate > 0:\n",
    "        mdist = cdist(data, [mu], metric='mahalanobis', V=Cov)[:,0]\n",
    "        x[mdist > truncate] = 0 # truncate\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mimic_augment(data, labels, plots=False, restarts=10, num_bins=0):\n",
    "    gen_points = np.empty((0, len(data[0])))\n",
    "    gen_labels = []\n",
    "    for l in np.unique(labels):\n",
    "        x = data[labels==l].astype(float)\n",
    "        if len(x) < 5: continue\n",
    "        cluster_clean = remove_outliers_lof(x)\n",
    "        if len(cluster_clean) < 5: continue\n",
    "        dev = np.std(cluster_clean, axis=0)\n",
    "        if np.sum(dev == 0) > 0:\n",
    "            for i in np.where(dev==0)[0]:\n",
    "                cluster_clean[:,i] += np.random.normal(0,1,len(cluster_clean))\n",
    "        \n",
    "        best_crit = np.Inf\n",
    "        for r in range(restarts):\n",
    "            ica_tmp = FastICA(n_components=len(data[0]), max_iter=500, tol=0.01).fit(cluster_clean.astype(float))\n",
    "            grids, fitted, fill_up, num_fill_up, _, _ = Imitate(ica_tmp.transform(cluster_clean), plots=plots, num_bins=num_bins)\n",
    "            #crit = 0 if sum(num_fill_up)==0 else sum(num_fill_up) / sum([sum(fill_up[i]>0)/len(fill_up[i]) for i in range(len(fill_up))])\n",
    "            crit = 0\n",
    "            for d in range(len(fitted)):\n",
    "                if sum(fill_up[d]) > 0:\n",
    "                    mids = [t + s for s, t in zip(grids[d], grids[d][1:])]\n",
    "                    mean_fitted = np.average(mids, weights=fitted[d])\n",
    "                    var_fitted = np.average((mids-mean_fitted)**2, weights=fitted[d])\n",
    "                    mean_fill = np.average(mids, weights=fill_up[d])\n",
    "                    var_fill = np.average((mids-mean_fill)**2, weights=fill_up[d])\n",
    "                    crit += var_fill / var_fitted\n",
    "            if plots: print(\"Round\", r, \":\", crit)\n",
    "            if crit < best_crit: \n",
    "                ica = copy.deepcopy(ica_tmp)\n",
    "                best_crit = crit\n",
    "                \n",
    "        data_trf = ica.transform(cluster_clean)\n",
    "        grids, fitted, fill_up, num_fill_up, _, _ = Imitate(data_trf, plots=plots, num_bins=num_bins)\n",
    "        num_gen = int(max(num_fill_up))\n",
    "        if num_gen == 0: continue\n",
    "        if plots: print(\"Final:\", best_crit, \"; generate\", num_gen)\n",
    "        points = np.empty((num_gen, 0))\n",
    "        \n",
    "        for d in range(len(data_trf[0])):\n",
    "            fill = fitted[d] / np.sum(fitted[d]) * (num_gen - num_fill_up[d])  +  fill_up[d] #mixed distr\n",
    "            fill_cdf = np.cumsum(fill) / num_gen  #normalize\n",
    "            \n",
    "            if plots:\n",
    "                mids = [t + s for s, t in zip(grids[d], grids[d][1:])]\n",
    "                plt.plot(mids, fitted[d], label=\"fitted\")\n",
    "                plt.plot(mids, fill_up[d], label=\"fill_up\")\n",
    "                plt.plot(mids, fill, label=\"fill\")\n",
    "                plt.plot(mids, fill_cdf, label=\"fill_cdf\")\n",
    "                plt.title(\"Class\"+str(l)+\"- Dim\"+str(d))\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "            \n",
    "            #generate points according to the cdf\n",
    "            vals = np.random.rand(num_gen)\n",
    "            val_bins = np.searchsorted(fill_cdf, vals)\n",
    "            coords = np.array([np.random.uniform(grids[d][val_bins[i]], grids[d][val_bins[i]+1]) \n",
    "                               for i in range(num_gen)]).reshape(num_gen, 1)\n",
    "            points = np.concatenate((points, coords), axis=1)\n",
    "            \n",
    "        gen_points = np.concatenate((gen_points, ica.inverse_transform(points)))\n",
    "        gen_labels = np.append(gen_labels, [l]*num_gen)\n",
    "            \n",
    "    return gen_points, gen_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(cluster_probs, cluster_params, data, quant_multiplier=10, olap_threshold=0.8, prints=False):\n",
    "    if len(cluster_params) <= 1: return cluster_probs, cluster_params\n",
    "    probs = np.copy(cluster_probs)\n",
    "    params = copy.deepcopy(cluster_params)\n",
    "    \n",
    "    # merge two clusters\n",
    "    def merge(a, b):\n",
    "        probs[:, a] = (probs[:, a] + probs[:, b]) / 2\n",
    "        probs[:, b] = 0\n",
    "        olaps[b, :] = olaps[:, b] = 0\n",
    "        member[a] = np.sum(probs[:, a])\n",
    "        member[b] = 0\n",
    "        params[a] = [(params[a][0] + params[b][0])/2, (params[a][1] + params[b][1])/2]\n",
    "        params[b] = None\n",
    "        for c in range(len(probs[0])):\n",
    "            if c==a: continue\n",
    "            olaps[a, c] = (member[a] - sum(probs[probs[:,a] > quant_multiplier*probs[:,c]][:,a])) / member[a]\n",
    "            olaps[c, a] = (member[c] - sum(probs[probs[:,c] > quant_multiplier*probs[:,a]][:,c])) / member[c]\n",
    "    \n",
    "    # remove overgrown clusters\n",
    "    nearest_clear_neighbor_dist = np.zeros(len(probs[0]))\n",
    "    for i in range(len(probs[0])):\n",
    "        clear_data = data[probs[:,i] > quant_multiplier*np.max(probs[:, np.delete(range(len(probs[0])), i)], axis=1)]\n",
    "        if len(clear_data) == 0: continue\n",
    "        distances = pairwise_distances(clear_data)\n",
    "        distances[distances==0] = np.Inf\n",
    "        nearest_clear_neighbor_dist[i] = np.average(np.min(distances, axis=0))\n",
    "    #dist = pairwise_distances(data)\n",
    "    #dist[dist==0] = np.Inf\n",
    "    #dist_min = np.min(dist, axis=0)\n",
    "    #avg_1nn_dist = np.average(dist_min) + np.std(dist_min)\n",
    "    #probs[:, np.where(nearest_clear_neighbor_dist > avg_1nn_dist)[0]] = 0\n",
    "    dist_threshold = np.average(nearest_clear_neighbor_dist) + 3*np.std(nearest_clear_neighbor_dist)\n",
    "    remove = np.where(nearest_clear_neighbor_dist > dist_threshold)[0]\n",
    "    probs[:, remove] = 0\n",
    "    for r in remove:\n",
    "        params[r] = None #delete parameters\n",
    "    \n",
    "    # quantify overlaps\n",
    "    olaps = np.empty((len(probs[0]), len(probs[0])))\n",
    "    member = np.sum(probs, axis=0)\n",
    "    for A in range(len(probs[0])):\n",
    "        for B in range(len(probs[0])):\n",
    "            if A == B:\n",
    "                olaps[A, B] = 0\n",
    "            else:\n",
    "                #olaps[A, B] = np.sum(np.multiply(probs[:,A], probs[:,A]-probs[:,B]))\n",
    "                clear_A = sum(probs[probs[:,A] > quant_multiplier*probs[:,B]][:,A])\n",
    "                olaps[A, B] = (member[A] - clear_A) / member[A]\n",
    "    \n",
    "    # merge\n",
    "    outer_flag = True\n",
    "    while outer_flag:\n",
    "        outer_flag = False\n",
    "        cl = prob_cluster_assignment(probs)\n",
    "        for A in range(len(olaps)):\n",
    "            if sum(cl==A) == 0: continue\n",
    "            A_flag = False\n",
    "            for B in range(A+1, len(olaps)):\n",
    "                if sum(cl==B) == 0: continue\n",
    "                if prints: print(\"Checking\", A, \"and\", B)\n",
    "                \n",
    "                # high symmetric overlap\n",
    "                if olaps[A, B] > olap_threshold and olaps[B, A] > olap_threshold:\n",
    "                    merge(A, B)\n",
    "                    A_flag = True\n",
    "                    break\n",
    "                \n",
    "                if olaps[A, B] > 0.2 or olaps[B, A] > 0.2:\n",
    "                    if merge_test(data, cl, A, B): \n",
    "                        merge(A, B)\n",
    "                        A_flag = True\n",
    "                        break\n",
    "                \n",
    "            if A_flag: \n",
    "                outer_flag = True\n",
    "                break\n",
    "    \n",
    "    # remove empty clusters\n",
    "    cl_sums = np.sum(cl, axis=0)\n",
    "    remove = np.where(cl_sums == 0)[0]\n",
    "    probs[:, remove] = 0\n",
    "    for r in remove:\n",
    "        params[r] = None\n",
    "        \n",
    "    return probs, list(filter(lambda p: p is not None, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test(data_, cl_, A, B):\n",
    "    dA, dB, dAB = data_[cl_==A], data_[cl_==B], data_[np.logical_or(cl_==A, cl_==B)]\n",
    "    dA_clean = remove_outliers_lof(dA) if len(dA) > 20 else dA\n",
    "    dB_clean = remove_outliers_lof(dB) if len(dB) > 20 else dB\n",
    "    dAB_clean = remove_outliers_lof(dAB) if len(dAB) > 20 else dAB\n",
    "    repeat = 10\n",
    "    norm_A = norm_B = norm_AB = 0\n",
    "    for k in range(repeat):\n",
    "        norm_A += 0 if len(dA)<5 else imitate_norm_test(dA_clean)\n",
    "        norm_B += 0 if len(dB)<5 else imitate_norm_test(dB_clean)\n",
    "        norm_AB += 0 if len(dAB)<5 else imitate_norm_test(dAB_clean)\n",
    "    norm_avg = (len(dA)*norm_A/repeat + len(dB)*norm_B/repeat) / len(dAB)\n",
    "    #print(\"Merge_test new result: \", norm_AB/repeat <= norm_avg)\n",
    "    return norm_AB/repeat <= norm_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging: Imitate as normality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imitate_norm_test(data):\n",
    "    if np.sum(np.std(data, axis=0) == 0) > 0: \n",
    "        data = data[:,np.std(data, axis=0)>0]\n",
    "    if len(data[0]) == 0: return 0\n",
    "    # train ICA on cluster, transform everything\n",
    "    ica = FastICA(n_components=len(data[0]), max_iter=500, tol=0.01).fit(data)\n",
    "    data_trf = ica.transform(data)\n",
    "    \n",
    "    # use Imitate\n",
    "    grids, fitted, fill_up, num_fill_up, _, _ = Imitate(data_trf, print_loss=False)\n",
    "    \n",
    "    #err = np.sum([sum(fill_up[i]) / sum(fitted[i]) for i in range(len(fitted))])\n",
    "    err = np.sum([sum(abs(fitted[i]-np.histogram(data_trf[:,i], bins=grids[i])[0])) / len(data) for i in range(len(fill_up))])\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_cluster_assignment(probs):\n",
    "    probs_norm = normalize(probs, norm='l1')\n",
    "    cl_choice = np.empty(len(probs))\n",
    "    for i in range(len(cl_choice)):\n",
    "        cl_choice[i] = -1 if sum(probs_norm[i])==0 else np.random.choice(range(len(probs[0])), p=probs_norm[i])\n",
    "    return cl_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========\n",
    "#      User Methods     \n",
    "# ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data, labels=None, centers=None, k_init=0, plots=False, full_plots=False):\n",
    "    if plots:\n",
    "        if labels is not None:\n",
    "            plotClusters(data, labels, title=\"Ground-Truth Clustering\")\n",
    "        elif centers is not None: \n",
    "            plt.scatter(data[:, 0], data[:, 1])\n",
    "            plt.scatter(centers[:,0], centers[:,1], c='red')\n",
    "            plt.title(\"Ground-Truth Clustering\")\n",
    "            plt.show()\n",
    "\n",
    "    if k_init==0: k_init = findK(data)\n",
    "    # params = mean/cov for each cluster\n",
    "    probs_imi, params = Mimic(data, k_init=k_init, cluster_plots=full_plots, full_plots=False)\n",
    "    if plots:\n",
    "        plotClusters(data, prob_cluster_assignment(probs_imi), title=\"raw results after ImiClust\")\n",
    "    \n",
    "    # merge the resulting clusters\n",
    "    probs_merge, params_merge = merge(probs_imi, params, data, prints=full_plots)\n",
    "    if plots:\n",
    "        labels_merge = prob_cluster_assignment(probs_merge)\n",
    "        data_clean = data[labels_merge >= 0]\n",
    "        labels_clean = labels_merge[labels_merge >= 0]\n",
    "        plotClusters(data_clean, labels_clean, title=\"Merged and cleaned results\")\n",
    "        \n",
    "    return probs_merge, params_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, params):\n",
    "    probs = np.column_stack([multivariate_normal(params[i][0], params[i][1]).pdf(data) for i in range(len(params))])\n",
    "    return prob_cluster_assignment(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(data, params, purge=True, plots=False):\n",
    "    labels = predict(data, params)\n",
    "    data_clean = data[labels >= 0]\n",
    "    labels_clean = labels[labels >= 0]\n",
    "        \n",
    "    points, point_labels = Mimic_augment(data_clean, labels_clean)\n",
    "    if plots: plotBias(np.concatenate((data, points)), np.append(labels, point_labels), range(len(data), len(data)+len(points)), title=\"Augmentation\")\n",
    "    if not purge: return points, point_labels\n",
    "    points_, point_labels_ = purge_low_confidence(data_clean, points, point_labels)\n",
    "    if plots: plotBias(np.concatenate((data, points_)), np.append(labels, point_labels_), range(len(data), len(data)+len(points_)), title=\"High Confidence Augmentation\")\n",
    "    return points_, point_labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of Cov:\n",
    "# Cholesky decomposition: for every real-valued symmetric positive-definite (SPD) matrix M, there is a \n",
    "#   unique lower-diagonal matrix L with positive diagonal entries and LL^T = M\n",
    "# => I generate lower-diagonal matrices m with positive diagonal and get Cov=mm^T !\n",
    "def generatePills(num_instances, num_clusters, num_dims, return_params=False, seed=None, mean_low=1, mean_high=100):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    points = np.empty((0,num_dims))\n",
    "    labels = []\n",
    "    params = []\n",
    "    num_cl = ([num_instances // num_clusters + (1 if x < num_instances % num_clusters else 0)  for x in range (num_clusters)])\n",
    "    for i in range(len(num_cl)):\n",
    "        \n",
    "        # generate Cov using Cholesky decomposition\n",
    "        m = rng.integers(1,50)*(2*rng.random((num_dims, num_dims))-1)\n",
    "        for j in range(len(m)):\n",
    "            m[j,j] = np.abs(m[j,j])\n",
    "        m = np.tril(m)\n",
    "        cov = m.dot(m.transpose())\n",
    "        \n",
    "        # generate mean\n",
    "        mean = rng.integers(mean_low,mean_high)*(2*rng.random(num_dims)-1)\n",
    "        \n",
    "        # sample points\n",
    "        pts = rng.multivariate_normal(mean, cov, size=num_cl[i])\n",
    "        \n",
    "        points = np.concatenate((points, pts), axis=0)\n",
    "        labels = np.append(labels, [i]*num_cl[i])\n",
    "        params.append([mean, cov])\n",
    "    if return_params: return points, np.array(labels), params\n",
    "    return points, np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBias(data, labels, num_biasedPills, prob=0.05, plots=False, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if plots: plotClusters(data, labels, title=\"Full data\")\n",
    "    delete_this = []\n",
    "    clusters = np.unique(labels)\n",
    "    if num_biasedPills > len(clusters): num_biasedPills = len(clusters)\n",
    "    \n",
    "    # select blobs that will be biased\n",
    "    bias_these = rng.choice(clusters, num_biasedPills, replace=False)\n",
    "    alphas = rng.random(num_biasedPills) * 2*np.pi # angle for plane\n",
    "    for blob, alpha in zip(bias_these, alphas):\n",
    "        dims = rng.choice(range(len(data[0])), 2, replace=False)\n",
    "        mean = data[labels == blob].mean(0)[dims]\n",
    "        d = np.sqrt(np.sum((data[:,dims] - mean)**2, axis=1))\n",
    "        angles = np.arcsin((data[:, dims[1]] - mean[1]) / d)\n",
    "        angles = np.array([(np.pi - angles[i] if data[i, dims[0]] < mean[0] else angles[i]) for i in range(len(angles))])\n",
    "        angles[angles < 0] += 2*np.pi\n",
    "        if alpha >= np.pi:\n",
    "            b = np.logical_or(angles > alpha, angles < alpha - np.pi)\n",
    "        else:\n",
    "            b = np.logical_and(angles > alpha, angles < (alpha + np.pi) % (2 * np.pi))\n",
    "        b = np.logical_and(b, labels == blob)\n",
    "        b = np.where(b)[0]\n",
    "        b = np.delete(b, rng.choice(range(len(b)), int(prob*len(b)), replace=False))\n",
    "        delete_this = np.append(delete_this, b)\n",
    "        \n",
    "    delete_this = delete_this.astype(int)\n",
    "    d, l = np.delete(data, delete_this, axis=0), np.delete(labels, delete_this)\n",
    "    if plots: plotClusters(d, l, title=\"Biased data\")\n",
    "    if plots: plotClusters(data[delete_this], labels[delete_this], title=\"Removed data\")\n",
    "    return d, l, delete_this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purge points with low confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge_low_confidence_perCluster(data, labels, points, point_labels):\n",
    "    delete = []\n",
    "    for cl in np.unique(point_labels):\n",
    "        idcs = np.where(point_labels == cl)[0]\n",
    "        if len(idcs) == 0 or sum(labels==cl) == 0: continue\n",
    "        if len(idcs) < 10 or sum(labels==cl) < 10: \n",
    "            delete = np.append(delete, idcs)\n",
    "            continue\n",
    "        nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(data[labels==cl])\n",
    "        avg_10nn = np.average(nbrs.kneighbors(data[labels==cl])[0])\n",
    "        sig_10nn = np.std(np.average(nbrs.kneighbors(data[labels==cl])[0]))\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(points[idcs])\n",
    "        points_10nn = np.average(nbrs.kneighbors(points[idcs])[0], axis=1)\n",
    "        delete = np.append(delete, idcs[points_10nn >= avg_10nn+sig_10nn])\n",
    "    if len(delete) == 0: return points, point_labels\n",
    "    delete = delete.astype(int)\n",
    "    return np.delete(points, delete, axis=0), np.delete(point_labels, delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge_low_confidence(data, points, point_labels):\n",
    "    delete = []\n",
    "    if len(data) < 10 or len(points) < 10: \n",
    "        return np.empty((0, len(data[0]))), []\n",
    "    nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(data)\n",
    "    avg_10nn = np.average(nbrs.kneighbors(data)[0])\n",
    "    sig_10nn = np.std(np.average(nbrs.kneighbors(data)[0]))\n",
    "        \n",
    "    nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(points)\n",
    "    points_10nn = np.average(nbrs.kneighbors(points)[0], axis=1)\n",
    "    keep = points_10nn < avg_10nn+sig_10nn\n",
    "    return points[keep, :], point_labels[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
